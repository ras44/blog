<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.54.0" />


<title>Calculating Always-valid p-values in R - Roland&#39;s Blog</title>
<meta property="og:title" content="Calculating Always-valid p-values in R - Roland&#39;s Blog">


  <link href='../../../favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../" class="nav-logo">
    <img src="../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="https://github.com/ras44">GitHub</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">8 min read</span>
    

    <h1 class="article-title">Calculating Always-valid p-values in R</h1>

    
    <span class="article-date">2019-08-04</span>
    

    <div class="article-content">
      <p><a href="https://github.com/ras44/blog/edit/master/content/post/2019-08-04-calculating-always-valid-p-values-in-r.Rmd">edit</a></p>

<p>In a <a href="https://ras44.github.io/blog/2019/04/08/validating-type-i-and-ii-errors-in-a-b-tests-in-r.html">previous post</a> we developed a framework for understanding Type-I and Type-II errors in A/B tests.  We learned how to properly calculate the number of observations per group for the test to have a particular false-positive-rate and false-negative rate.  We also learned that the correct time to measure the results of such a test is when exactly the calculated number of observations have been made: one time per experiment.  Finally, we learned that a common problem plaguing A/B tests is <strong>peeking</strong>.</p>

<blockquote>
<blockquote>
<p><strong>Peeking</strong>: visually or programatically deciding the result of an A/B test before the pre-calculated number of observations have been observed.  Can lead to highly inflated false positive rates.</p>
</blockquote>
</blockquote>

<p>In this post, we will develop a framework for always-valid inference based on the paper <a href="https://arxiv.org/pdf/1512.04922.pdf">Always Valid Inference: Continuous Monitoring of A/B Tests</a> (2019 Johari, Pekelis, Walsh).  Using an always-valid p-value allows us to continuously monitor A/B tests and potentially stop the test early in a valid way<sup class="footnote-ref" id="fnref:1"><a href="#fn:1">1</a></sup>.</p>

<p>In section 5 of the paper the authors propose their method for calculating always-valid p-values: the minimal sequential ratio probability test (mSPRT), first introduced by Robbins (1970).  To keep this post brief, we will not do the paper&rsquo;s theoretical foundations justice.  Instead we will focus on the most important equations, which we will use to produce always valid p-values in our R-code.  For those uninterested in the math, feel free to skip ahead.</p>

<p>The paper makes some basic assumptions on the data and its functional form: it is real valued and drawn from a single-parameter exponential family where tests are of the parameter <code>\(\theta\)</code>.  The mSPRT is parametrized by a mixing distribution <code>\(H\)</code> over <code>\(\Theta\)</code>, an open interval that contains all <code>\(\theta\)</code>.  Given an observed sample average <code>\(s_n\)</code> at time <code>\(n\)</code>, the mixture likelihood ratio of <code>\(\theta\)</code> against <code>\(\theta_0\)</code> with respect to H is defined as:</p>

<p><code>\(\Lambda_{n}^{H}(s_{n}) = \displaystyle\int_{\Theta}\bigg(\frac{f_{\theta}(s_{n})}{f_{\theta_{0}}(s_{n})}\bigg)^{n} dH(\theta)\)</code></p>

<p>We are told to calculate the p-values as:</p>

<p><code>\(p_{0} = 1; p_{n} = min\{p_{n-1}, 1/\Lambda_{n}^{H}\}\)</code></p>

<p>So, if we have a mixing distribution <code>\(H(\theta)\)</code> and integrate over it, we can calculate <code>\(\Lambda_{n}^{H}(s_{n})\)</code> for the sample average we have observed at time <code>\(n\)</code>.  If <code>\(\Lambda_{n}^{H}\)</code> is ever greater than <code>\(\alpha\)</code>, our tolerance for the false positive rate, <code>\(p_n\)</code> will exceed <code>\(alpha\)</code> and we can stop the test and reject the null hypothesis.</p>

<p>We will implement this for a simple comparison of two binomial distributions first, where some convenient results make <code>\(p_{n}\)</code> easier to calculate.  In section 6.1, the paper describes the deployment for two-stream p-values.  In this case, we are looking at two streams of <em>Bernoulli</em> data: two streams of ones and zeros describing whether we observed a conversion or an abandonment in each stream.</p>

<p>The <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a> tells us that the distribution of the average number of conversions will be approximately normal after a large number of observations (say, more than 100).  What we want to know is: is there a statistically significant difference between the measured averages of the two streams?  Put another way: is the difference between the measured averages of the two streams significantly different from zero?  It is this second formulation that we will implement.</p>

<p>We will skip the derivation for normal data and say that if we have two streams of Bernoulli data, <code>\(A_n\)</code> and <code>\(B_n\)</code>, that each yield approximately normal average conversion rates <code>\(\mu_{A,n}\)</code> and <code>\(\mu_{B,n}\)</code> after <code>\(n\)</code> observations, we can take <code>\(f_\theta(s_n) \approx N(\theta, \sigma^2)\)</code> and use a normal mixing distribution <code>\(H=N(0,\tau^2)\)</code> in <code>\(\Lambda_n^H\)</code> to decide if <code>\(\theta_n\)</code> is significantly different from zero.  Conveniently, this mixing distribution yields a closed-form formula for <code>\(\Lambda_n^H\)</code>:</p>

<p><code>\(\Lambda_n^H = \sqrt{\frac{\sigma_n^2}{\sigma_n^2+n\tau^2}} exp\bigg\{\frac{n^2\tau^2(\theta_n)^2}{2\sigma_n^2(\sigma_n^2+n\tau^2)}\bigg\}\)</code></p>

<p>Let&rsquo;s look at the variables in the above equation: We can calculate <code>\(\theta_n = \mu_{A,n}-\mu_{B,n}\)</code> from our streams of Bernoulli data by calculating the average conversion rates at <code>\(n\)</code> for each stream.  We can also use the fact that the variance <code>\(\nu\)</code> of a binomial distribution with mean <code>\(\mu\)</code> is <code>\(\nu = \mu(1-\mu)\)</code> and that the variance of the sum of two random binomials is the sum of their variances:</p>

<p><code>\(\nu_{AB} = \nu_A + \nu_B = \mu_{A,n}(1-\mu_{A,n}) + \mu_{B,n}(1-\mu_{B,n}) = \sigma_n^2\)</code>.</p>

<p>One lingering issue is: how do we define <code>\(\tau^2\)</code>? The answer is we can choose any <code>\(\tau^2\)</code> we would like, though experience will show it is best if <code>\(\tau^2\)</code> is on the order of <code>\(\sigma^2\)</code>.  Later we will explore what different <code>\(\tau^2\)</code> values yield, and how there is an optimal <code>\(\tau^2\)</code> to choose.</p>

<p>Let&rsquo;s convert the above into code:</p>

<pre><code class="language-r"># First we define the function that calculates lambda
# given n, the means at n of A and B, and tau_squared 
lambda &lt;- function(n, mu_a_n, mu_b_n, tau_sq){
  v_n &lt;- (mu_a_n*(1-mu_a_n) + mu_b_n*(1-mu_b_n))
  nts &lt;- n*tau_sq
  if(v_n == 0){
    return(1.0)
  }
  else {
    return(
      sqrt((v_n)/(v_n+nts))*
        exp(
          ((n*nts)*(mu_a_n-mu_b_n)^2)/
            ((2.0*v_n)*(v_n+nts))
        )
    )
  }
}

# Next we will calculate the always valid p-values at each n
calc_avpvs &lt;- function(n_obs, cr_a_obs, cr_b_obs, tau_sq = 0.1){
  p_n &lt;- rep(1.0,n_obs)

  for (i in 2:n_obs) {
    mu_a_n &lt;- mean(cr_a_obs[1:i])
    mu_b_n &lt;- mean(cr_b_obs[1:i])
    p_n[[i]] &lt;- min(p_n[[i-1]],1/lambda(i,mu_a_n, mu_b_n, tau_sq))
  }
  
  p_n
}
</code></pre>

<p>Now, let&rsquo;s create a test example, similar to our previous post, and set an effect size that is twice the minimum detectable effect.  In this case, we should expect that after having observed the &ldquo;correct&rdquo; number of observations given by the power calculation, there is a very high probability that we&rsquo;ll see an effect.</p>

<pre><code class="language-r">library(pwr)
library(ggplot2)
set.seed(5)

mde &lt;- 0.1  # minimum detectable effect
cr_a &lt;- 0.25 # the expected conversion rate for group A
alpha &lt;- 0.05 # the false positive rate
power &lt;- 0.80 # 1-false negative rate

ptpt &lt;- pwr.2p.test(h = ES.h(p1 = cr_a, p2 = (1+mde)*cr_a), 
                    sig.level = alpha, 
                    power = power
)
n_obs &lt;- ceiling(ptpt$n)

# make our &quot;true&quot; effect 1.5x larger than the mde
# this should yield a conclusive test result
effect &lt;- 1.5*mde
cr_b &lt;- (1+effect)*cr_a
observations &lt;- 2*n_obs

# two streams of {0,1} conversions
conversions_a &lt;- rbinom(observations, 1, cr_a)
conversions_b &lt;- rbinom(observations, 1, cr_b)

# now we'll calculate the always-valid p-values
avpvs &lt;- calc_avpvs(observations, conversions_a, conversions_b)


# And we'll calculate &quot;regular&quot; p-values as well
tt &lt;- sapply(10:observations, function(x){
  t.test(conversions_a[1:x],conversions_b[1:x])$p.value
})

tt &lt;- data.frame(p.value = unlist(tt))

# for plots
conf_95 &lt;- data.frame( x = c(-Inf, Inf), y = 0.95 )
obs_limit_line &lt;- data.frame( x = n_obs, y = c(-Inf, Inf) )

# plot the evolution of p-values and always-valid p-values
ggplot(tt, aes(x=seq_along(p.value), y=1-p.value)) + 
  geom_line() + 
  geom_line(aes(x, y, color=&quot;alpha=5%&quot;), linetype=3, conf_95) + 
  geom_line(aes(x, y, color=&quot;end of test&quot;), linetype=4, obs_limit_line) +
  geom_line(data=data.frame(x=seq(1:observations),y=1-avpvs), aes(x=x,y=y, color=&quot;avpv&quot;)) +
  xlab(&quot;Observation (n)&quot;) +
  scale_color_discrete(name = &quot;Legend&quot;) +
  ylim(c(0,1))
</code></pre>

<p><img src="../../../post/2019-08-04-calculating-always-valid-p-values-in-r_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>

<p>In this case, we see that by using the always valid p-value we would be able to terminate early while still controlling the false positive rate at <code>\(\alpha\)</code>.  Let&rsquo;s look at what happens when there is no effect:</p>

<pre><code class="language-r">effect &lt;- 0
cr_b &lt;- (1+effect)*cr_a
observations &lt;- 2*n_obs

# two streams of {0,1} conversions
conversions_a &lt;- rbinom(observations, 1, cr_a)
conversions_b &lt;- rbinom(observations, 1, cr_b)

# now we'll calculate the always-valid p-values
avpvs &lt;- calc_avpvs(observations, conversions_a, conversions_b)

# And we'll calculate &quot;regular&quot; p-values as well
tt &lt;- sapply(10:observations, function(x){
  t.test(conversions_a[1:x],conversions_b[1:x])$p.value
})
tt &lt;- data.frame(p.value = unlist(tt))

# for plots
conf_95 &lt;- data.frame( x = c(-Inf, Inf), y = 0.95 )
obs_limit_line &lt;- data.frame( x = n_obs, y = c(-Inf, Inf) )

# plot the evolution of p-values and always-valid p-values
ggplot(tt, aes(x=seq_along(p.value), y=1-p.value)) + 
  geom_line() + 
  geom_line(aes(x, y, color=&quot;alpha=5%&quot;), linetype=3, conf_95) + 
  geom_line(aes(x, y, color=&quot;end of test&quot;), linetype=4, obs_limit_line) +
  geom_line(data=data.frame(x=seq(1:observations),y=1-avpvs), aes(x=x,y=y, color=&quot;avpv&quot;)) +
  xlab(&quot;Observation (n)&quot;) +
  scale_color_discrete(name = &quot;Legend&quot;) +
  ylim(c(0,1))
</code></pre>

<p><img src="../../../post/2019-08-04-calculating-always-valid-p-values-in-r_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>

<p>In this case, we see a test in which the p-values oscillate near alpha.  A &ldquo;peeker&rdquo; looking at this might have incorrectly called it a winner early on, however our always-valid p-values maintain the correct outcome.</p>

<p>Now set things up to have an effect 1.5 times the size of the minimum detectable effect and see what effect varying <code>\(\tau^2\)</code> has.  We&rsquo;ll choose 5 different values at different orders of magnitude between 0.0001 and 1:</p>

<pre><code class="language-r">library(pwr)
library(ggplot2)
set.seed(5)

mde &lt;- 0.1  # minimum detectable effect
cr_a &lt;- 0.25 # the expected conversion rate for group A
alpha &lt;- 0.05 # the false positive rate
power &lt;- 0.80 # 1-false negative rate

ptpt &lt;- pwr.2p.test(h = ES.h(p1 = cr_a, p2 = (1+mde)*cr_a), 
                    sig.level = alpha, 
                    power = power
)
n_obs &lt;- ceiling(ptpt$n)
n_obs
</code></pre>

<pre><code>## [1] 4860
</code></pre>

<pre><code class="language-r"># make our &quot;true&quot; effect 1.5x larger than the mde
# this should yield a conclusive test result
effect &lt;- 1.5*mde
cr_b &lt;- (1+effect)*cr_a
observations &lt;- 2*n_obs

# two streams of {0,1} conversions
conversions_a &lt;- rbinom(observations, 1, cr_a)
conversions_b &lt;- rbinom(observations, 1, cr_b)

# now we'll calculate the always-valid p-values
avpvs_tsa &lt;- calc_avpvs(observations, conversions_a, conversions_b, tau_sq = 0.0001)
avpvs_tsb &lt;- calc_avpvs(observations, conversions_a, conversions_b, tau_sq = 0.001)
avpvs_tsc &lt;- calc_avpvs(observations, conversions_a, conversions_b, tau_sq = 0.01)
avpvs_tsd &lt;- calc_avpvs(observations, conversions_a, conversions_b, tau_sq = 0.1)
avpvs_tse &lt;- calc_avpvs(observations, conversions_a, conversions_b, tau_sq = 1)

# And we'll calculate &quot;regular&quot; p-values as well
tt &lt;- sapply(10:observations, function(x){
  t.test(conversions_a[1:x],conversions_b[1:x])$p.value
})
tt &lt;- data.frame(p.value = unlist(tt))

# for plots
conf_95 &lt;- data.frame( x = c(-Inf, Inf), y = 0.95 )
obs_limit_line &lt;- data.frame( x = n_obs, y = c(-Inf, Inf) )

# plot the evolution of p-values and always-valid p-values
ggplot(tt, aes(x=seq_along(p.value), y=1-p.value)) + 
  geom_line() + 
  geom_line(aes(x, y, color=&quot;alpha=5%&quot;), linetype=3, conf_95) + 
  geom_line(aes(x, y, color=&quot;end of test&quot;), linetype=4, obs_limit_line) +
  geom_line(data=data.frame(x=seq(1:observations),y=1-avpvs_tsa), aes(x=x,y=y, color=&quot;avpv_0.0001&quot;)) +
  geom_line(data=data.frame(x=seq(1:observations),y=1-avpvs_tsb), aes(x=x,y=y, color=&quot;avpv_0.001&quot;)) +
  geom_line(data=data.frame(x=seq(1:observations),y=1-avpvs_tsc), aes(x=x,y=y, color=&quot;avpv_0.01&quot;)) +
  geom_line(data=data.frame(x=seq(1:observations),y=1-avpvs_tsd), aes(x=x,y=y, color=&quot;avpv_0.1&quot;)) +
  geom_line(data=data.frame(x=seq(1:observations),y=1-avpvs_tse), aes(x=x,y=y, color=&quot;avpv_1&quot;)) +
  xlab(&quot;Observation (n)&quot;) +
  scale_color_discrete(name = &quot;Legend&quot;) +
  ylim(c(0,1))
</code></pre>

<p><img src="../../../post/2019-08-04-calculating-always-valid-p-values-in-r_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>

<p>It looks like we might be able to stop early if we use anything by the lowest and highest values for <code>\(\tau^2\)</code>.</p>

<p>Using always-valid p-values, particularly for small or zero-sized effects, can be helpful in avoiding false positives.  I&rsquo;ve glossed over some important details that should be considered when working with non-normal data, but I hope this provides an introduction to an alternative way of evaluating binomial A/B tests.  See <a href="https://arxiv.org/pdf/1512.04922.pdf">Always Valid Inference: Continuous Monitoring of A/B Tests</a> for more details.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">For an even simpler approach, see what <a href="https://codeascraft.com/2018/10/03/how-etsy-handles-peeking-in-a-b-testing/">Etsy does</a>.
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
</ol>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../../../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../../../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

