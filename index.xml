<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Roland&#39;s Blog</title>
    <link>/</link>
    <description>Recent content on Roland&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Apr 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>10 Commands to Get Started with Git</title>
      <link>/2020/04/15/10-commands-to-get-started-with-git.html</link>
      <pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/15/10-commands-to-get-started-with-git.html</guid>
      <description>edit
Git and its online extensions like GitHub, Bitbucket, and GitLab are essential tools for data science. While the emphasis is often on collaboration, Git can also be very useful to the solo practitioner.
RStudio offers Git functionality via a convenient web-based interface (see the “Git” tab), as well as interaction with git via the command-line (via the “Console” tab, or via the “Git” tab’s “More”-&amp;gt;“Shell” menu option).
Resources such as HappyGitWithR and try.</description>
    </item>
    
    <item>
      <title>Git Workflows for Reproducible Research</title>
      <link>/2020/02/24/git-workflows-for-reproducible-research.html</link>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/24/git-workflows-for-reproducible-research.html</guid>
      <description>edit
About 10 years ago, Vincent Driessen published the seminal post “A successful Git branching model” describing a logical process for developing, releasing, and fixing software projects using Git branches. It quickly became knows as Gitflow, and is one of the most widely used branching models for release-based software development.
The essential branches in the Gitflow branching model
 Gitflow was perhaps the first well-publicized Git branching model. Other branching models followed, the most famous being GitHub Flow, which proposed a more simplified philosophy aimed at software deployed to production continuously.</description>
    </item>
    
    <item>
      <title>In-Database Logistic Regression with R</title>
      <link>/2019/11/05/in-database-logistic-regression-with-r.html</link>
      <pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/05/in-database-logistic-regression-with-r.html</guid>
      <description>edit
In a previous article we illustrated how to calculate xgboost model predictions in-database. This was referenced and incorporated into tidypredict. After learning more about what the tidypredict team are up to, I discovered another tidyverse package called modeldb that fits models in-database. It currently supports linear regression and k-means clustering, so I thought I would provide an example of how to do in-database logistic regression.
Rather than focusing on the details of logistic regression, we will focus more on how we can use R and some carefully written SQL statements iteratively minimize a cost function.</description>
    </item>
    
    <item>
      <title>Multiple Hypothesis Testing in R</title>
      <link>/2019/09/20/multiple-hypothesis-testing-in-r.html</link>
      <pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/20/multiple-hypothesis-testing-in-r.html</guid>
      <description>edit
In the first article of this series we looked at understanding type I and type II errors in the context of an A/B test and highlighted the issue of “peeking”. In the second, we illustrated a way to calculate always-valid p-values that were immune to peeking. We will now explore multiple hypothesis testing: what happens when multiple tests are conducted on the same family of data.
We will set things up as before, with the false positive rate \(\alpha = 0.</description>
    </item>
    
    <item>
      <title>Calculating Always-valid p-values in R</title>
      <link>/2019/08/04/calculating-always-valid-p-values-in-r.html</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/04/calculating-always-valid-p-values-in-r.html</guid>
      <description>edit
In a previous post we developed a framework for understanding Type-I and Type-II errors in A/B tests. We learned how to properly calculate the number of observations per group for the test to have a particular false-positive-rate and false-negative rate. We also learned that the correct time to measure the results of such a test is when exactly the calculated number of observations have been made: one time per experiment.</description>
    </item>
    
    <item>
      <title>Reticulate, virtualenv, and Python in linux</title>
      <link>/2019/05/04/reticulate-virtualenv-and-python-in-linux-demo.html</link>
      <pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/04/reticulate-virtualenv-and-python-in-linux-demo.html</guid>
      <description>edit
Motivation reticulate is an R-package that allows us to use Python modules from within RStudio. I recently found this functionality useful while trying to compare the results of different uplift models. Though I did have R’s uplift package producing Qini charts and metrics, I also wanted to see how things looked with Wayfair’s promising pylift package. Since pylift is only available in python, reticulate made it easy for me to quickly use pylift from within RStudio.</description>
    </item>
    
    <item>
      <title>Validating Type I and II Errors in A/B Tests in R</title>
      <link>/2019/04/08/validating-type-i-and-ii-errors-in-a-b-tests-in-r.html</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/08/validating-type-i-and-ii-errors-in-a-b-tests-in-r.html</guid>
      <description>edit
In the below work, we will intentionally leave out statistics theory and attempt to develop an intuitive sense of what type I(false-positive) and type II(false-negative) errors represent when comparing metrics in A/B tests.
One of the problems plaguing the analysis of A/B tests today is known as the “peeking problem”. To better understand what “peeking” is, it helps to first understand how to properly run a test. We will focus on the case of testing whether there is a difference between the conversion rates cr_a and cr_b for groups A and B.</description>
    </item>
    
    <item>
      <title>Understanding how Optimizely Controls Error in A/B Tests with R Part 1</title>
      <link>/2019/03/25/understanding-how-optimizely-controls-error-in-a-b-tests-with-r.html</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/25/understanding-how-optimizely-controls-error-in-a-b-tests-with-r.html</guid>
      <description>edit
 I’m neither an employee nor paid-promoter of Optimizely. The views expressed are my own.
 Optimizely is popular testing platform that makes understanding the results of A/B tests easier for anyone who isn’t well-versed in statistical testing methodologies (and even those who are!). In the below work, we will intentionally leave out statistics theory and attempt to develop an intuitive sense of what is described in Optimizely’s “Always Valid Inference” and “Peeking at A/B Tests” papers.</description>
    </item>
    
    <item>
      <title>Keeping Credentials Secret with Keyrings in R</title>
      <link>/2019/01/19/keeping-credentials-secret-with-keyrings-in-r.html</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/19/keeping-credentials-secret-with-keyrings-in-r.html</guid>
      <description>edit
Keeping credentials secret When accessing an API or database in R, it is often necessary to provide credentials such as login name and password.
Figure: Providing credentials via an interactive prompt
 Often it is also more convenient to provide credentials programatically in an R script, but best practices1 state:
 As with every programming language, it is important to avoid publishing code with your credentials in plain text.</description>
    </item>
    
    <item>
      <title>Setting up RStudio Server on a cloud with Linux.</title>
      <link>/2019/01/05/using-a-fixed-version-of-r-on-linux.html</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/05/using-a-fixed-version-of-r-on-linux.html</guid>
      <description>edit
Roland Stevenson is a data scientist and consultant who may be reached on Linkedin.
When setting up R and RStudio Server on a cloud Linux instance, some thought should be given to implementing a workflow that facilitates collaboration and ensures R project reproducibility. There are many possible workflows to accomplish this. In this post we offer an “opinionated” solution based on what we have found to work in a production environment.</description>
    </item>
    
    <item>
      <title>In-database xgboost Predictions with R</title>
      <link>/2018/10/18/in-database-xgboost-predictions-with-r.html</link>
      <pubDate>Thu, 18 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/18/in-database-xgboost-predictions-with-r.html</guid>
      <description>edit
xgboost(docs) is a popular R package for classification and regression, and the model of choice in many winning Kaggle competitions. Moving xgboost into a large-scale production environment, however, can lead to challenges when attempting to calculate predictions (“scores”) for large datasets. We present a novel solution for calculating batch predictions without having to transfer features stored in a database to the machine where the model is located; instead we convert the model predictions into SQL commands and thereby transfer the scoring process to the database.</description>
    </item>
    
    <item>
      <title>Cost Effective Partitioning in BigQuery with R</title>
      <link>/2018/05/02/cost-effective-partitioning-in-bigquery-with-r.html</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/02/cost-effective-partitioning-in-bigquery-with-r.html</guid>
      <description>edit
Introduction Companies using Google BigQuery for production analytics often run into the following problem: the company has a large user hit table that spans many years. Since queries are billed based on the fields accessed, and not on the date-ranges queried, queries on the table are billed for all available days and are increasingly wasteful.
Partitioning Tables
 A solution is to partition the table by date, so that users can query a particular range of dates; saving costs and decreasing query duration.</description>
    </item>
    
  </channel>
</rss>