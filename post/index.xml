<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Roland&#39;s Blog</title>
    <link>/post.html</link>
    <description>Recent content in Posts on Roland&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Apr 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Validating Type I and II Errors in A/B Tests in R</title>
      <link>/2019/04/08/validating-type-i-and-ii-errors-in-a-b-tests-in-r.html</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/08/validating-type-i-and-ii-errors-in-a-b-tests-in-r.html</guid>
      <description>edit
In the below work, we will intentionally leave out statistics theory and attempt to develop an intuitive sense of what type I and type II errors represent when running A/B tests.
One of the problems plaguing the analysis of A/B tests today is known as the “peeking problem”. To better understand what “peeking” is, it helps to first understand how to properly run a test. We will focus on the case of testing whether there is a difference between the conversion rates cr_a and cr_b for groups A and B.</description>
    </item>
    
    <item>
      <title>Understanding how Optimizely Controls Error in A/B Tests with R Part 1</title>
      <link>/2019/03/25/understanding-how-optimizely-controls-error-in-a-b-tests-with-r.html</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/25/understanding-how-optimizely-controls-error-in-a-b-tests-with-r.html</guid>
      <description>edit
 I’m neither an employee nor paid-promoter of Optimizely. The views expressed are my own.
 Optimizely is popular testing platform that makes understanding the results of A/B tests easier for anyone who isn’t well-versed in statistical testing methodologies (and even those who are!). In the below work, we will intentionally leave out statistics theory and attempt to develop an intuitive sense of what is described in Optimizely’s “Always Valid Inference” and “Peeking at A/B Tests” papers.</description>
    </item>
    
    <item>
      <title>Keeping Credentials Secret with Keyrings in R</title>
      <link>/2019/01/19/keeping-credentials-secret-with-keyrings-in-r.html</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/19/keeping-credentials-secret-with-keyrings-in-r.html</guid>
      <description>edit
Keeping credentials secret When accessing an API or database in R, it is often necessary to provide credentials such as login name and password.
Figure: Providing credentials via an interactive prompt
 Often it is also more convenient to provide credentials programatically in an R script, but best practices1 state:
 As with every programming language, it is important to avoid publishing code with your credentials in plain text.</description>
    </item>
    
    <item>
      <title>Using a Fixed Version of R on Linux</title>
      <link>/2019/01/05/using-a-fixed-version-of-r-on-linux.html</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/05/using-a-fixed-version-of-r-on-linux.html</guid>
      <description>edit
 The below article refers to setting up R and RStudio Server on a cloud Linux instance in a way that ensures R project reproducibility and facilitates collaboration. Many possible workflows exist to accomplish this. One might call the below presentation an “opinionated” solution based on what we have found to work in a production environment. Importantly, all development is on an RStudio Server cloud Linux instance, ensuring that we only have to support one operating system.</description>
    </item>
    
    <item>
      <title>In-database xgboost Predictions with R</title>
      <link>/2018/10/18/in-database-xgboost-predictions-with-r.html</link>
      <pubDate>Thu, 18 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/18/in-database-xgboost-predictions-with-r.html</guid>
      <description>edit
xgboost(docs) is a popular R package for classification and regression, and the model of choice in many winning Kaggle competitions. Moving xgboost into a large-scale production environment, however, can lead to challenges when attempting to calculate predictions (“scores”) for large datasets. We present a novel solution for calculating batch predictions without having to transfer features stored in a database to the machine where the model is located; instead we convert the model predictions into SQL commands and thereby transfer the scoring process to the database.</description>
    </item>
    
    <item>
      <title>Cost Effective Partitioning in BigQuery with R</title>
      <link>/2018/05/02/cost-effective-partitioning-in-bigquery-with-r.html</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/02/cost-effective-partitioning-in-bigquery-with-r.html</guid>
      <description>edit
Introduction Companies using Google BigQuery for production analytics often run into the following problem: the company has a large user hit table that spans many years. Since queries are billed based on the fields accessed, and not on the date-ranges queried, queries on the table are billed for all available days and are increasingly wasteful.
Partitioning Tables
 A solution is to partition the table by date, so that users can query a particular range of dates; saving costs and decreasing query duration.</description>
    </item>
    
  </channel>
</rss>